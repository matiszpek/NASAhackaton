{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install numpy pandas matplotlib seaborn scikit-learn tensorflow obspy distutils obspy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file 1 of 76\n",
      "Processing file 2 of 76\n",
      "Processing file 3 of 76\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kono\\AppData\\Local\\Temp\\ipykernel_11592\\1764884676.py:55: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, pd.DataFrame([temp_dict])], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file 4 of 76\n",
      "Processing file 5 of 76\n",
      "Processing file 6 of 76\n",
      "Processing file 7 of 76\n",
      "Processing file 8 of 76\n",
      "Processing file 9 of 76\n",
      "Processing file 10 of 76\n",
      "Processing file 11 of 76\n",
      "Processing file 12 of 76\n",
      "Processing file 13 of 76\n",
      "Processing file 14 of 76\n",
      "Processing file 15 of 76\n",
      "Processing file 16 of 76\n",
      "Processing file 17 of 76\n",
      "Processing file 18 of 76\n",
      "Processing file 19 of 76\n",
      "Processing file 20 of 76\n",
      "Processing file 21 of 76\n",
      "Processing file 22 of 76\n",
      "Processing file 23 of 76\n",
      "Processing file 24 of 76\n",
      "Processing file 25 of 76\n",
      "Processing file 26 of 76\n",
      "Processing file 27 of 76\n",
      "Processing file 28 of 76\n",
      "Processing file 29 of 76\n",
      "Processing file 30 of 76\n",
      "Processing file 31 of 76\n",
      "Processing file 32 of 76\n",
      "Processing file 33 of 76\n",
      "Processing file 34 of 76\n",
      "Processing file 35 of 76\n",
      "Processing file 36 of 76\n",
      "Processing file 37 of 76\n",
      "Processing file 38 of 76\n",
      "Processing file 39 of 76\n",
      "Processing file 40 of 76\n",
      "Processing file 41 of 76\n",
      "Processing file 42 of 76\n",
      "Processing file 43 of 76\n",
      "Processing file 44 of 76\n",
      "Processing file 45 of 76\n",
      "Processing file 46 of 76\n",
      "Processing file 47 of 76\n",
      "Processing file 48 of 76\n",
      "Processing file 49 of 76\n",
      "Processing file 50 of 76\n",
      "Processing file 51 of 76\n",
      "Processing file 52 of 76\n",
      "Processing file 53 of 76\n",
      "Processing file 54 of 76\n",
      "Processing file 55 of 76\n",
      "Processing file 56 of 76\n",
      "Processing file 57 of 76\n",
      "Processing file 58 of 76\n",
      "Processing file 59 of 76\n",
      "Processing file 60 of 76\n",
      "Processing file 61 of 76\n",
      "Processing file 62 of 76\n",
      "Processing file 63 of 76\n",
      "Processing file 64 of 76\n",
      "Processing file 65 of 76\n",
      "Processing file 66 of 76\n",
      "Processing file 67 of 76\n",
      "Processing file 68 of 76\n",
      "Processing file 69 of 76\n",
      "Processing file 70 of 76\n",
      "Processing file 71 of 76\n",
      "Processing file 72 of 76\n",
      "Processing file 73 of 76\n",
      "Processing file 74 of 76\n",
      "Processing file 75 of 76\n",
      "Processing file 76 of 76\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>start</th>\n",
       "      <th>id</th>\n",
       "      <th>cant_measurements</th>\n",
       "      <th>st</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>xa.s12.00.mhz.1970-01-19HR00_evid00002</td>\n",
       "      <td>73500.0</td>\n",
       "      <td>00002</td>\n",
       "      <td>572415</td>\n",
       "      <td>[(-6.153278962788711e-14, -7.70128843364098e-1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>xa.s12.00.mhz.1970-03-25HR00_evid00003</td>\n",
       "      <td>12720.0</td>\n",
       "      <td>00003</td>\n",
       "      <td>572411</td>\n",
       "      <td>[(-5.481780117043957e-15, -6.8786525555433944e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>xa.s12.00.mhz.1970-03-26HR00_evid00004</td>\n",
       "      <td>73020.0</td>\n",
       "      <td>00004</td>\n",
       "      <td>572411</td>\n",
       "      <td>[(-2.8212463353274306e-14, -3.523317065258157e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>xa.s12.00.mhz.1970-04-25HR00_evid00006</td>\n",
       "      <td>4440.0</td>\n",
       "      <td>00006</td>\n",
       "      <td>572415</td>\n",
       "      <td>[(9.01642264710853e-15, 1.1305708384819468e-14...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>xa.s12.00.mhz.1970-04-26HR00_evid00007</td>\n",
       "      <td>52140.0</td>\n",
       "      <td>00007</td>\n",
       "      <td>572411</td>\n",
       "      <td>[(-1.5835653822406575e-16, -1.8729952083938931...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                file_name    start     id cant_measurements  \\\n",
       "0  xa.s12.00.mhz.1970-01-19HR00_evid00002  73500.0  00002            572415   \n",
       "1  xa.s12.00.mhz.1970-03-25HR00_evid00003  12720.0  00003            572411   \n",
       "2  xa.s12.00.mhz.1970-03-26HR00_evid00004  73020.0  00004            572411   \n",
       "3  xa.s12.00.mhz.1970-04-25HR00_evid00006   4440.0  00006            572415   \n",
       "4  xa.s12.00.mhz.1970-04-26HR00_evid00007  52140.0  00007            572411   \n",
       "\n",
       "                                                  st  \n",
       "0  [(-6.153278962788711e-14, -7.70128843364098e-1...  \n",
       "1  [(-5.481780117043957e-15, -6.8786525555433944e...  \n",
       "2  [(-2.8212463353274306e-14, -3.523317065258157e...  \n",
       "3  [(9.01642264710853e-15, 1.1305708384819468e-14...  \n",
       "4  [(-1.5835653822406575e-16, -1.8729952083938931...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from obspy import read\n",
    "import os\n",
    "import sys\n",
    "\n",
    "\n",
    "# Define el directorio de datos\n",
    "path_to_dataset = r'C:\\Users\\Kono\\Desktop\\space_apps_2024_seismic_detection\\data'\n",
    "sys.path.append(path_to_dataset)\n",
    "data_directory = path_to_dataset + r'\\lunar\\training\\data\\S12_GradeA'\n",
    "data_files = os.listdir(data_directory)\n",
    "data_files = [data_directory + '\\\\' + file for file in data_files if file.endswith('.mseed')]\n",
    "abstract_dfs = pd.read_csv(r'C:\\Users\\Kono\\Desktop\\space_apps_2024_seismic_detection\\data\\lunar\\training\\catalogs\\apollo12_catalog_GradeA_final.csv')\n",
    "\n",
    "# Inicializa un DataFrame vacío con columnas especificadas\n",
    "df = pd.DataFrame(columns=['file_name', 'start', 'id', 'cant_measurements', 'st'])\n",
    "\n",
    "for index, file in enumerate(data_files):\n",
    "    print(f'Processing file {index+1} of {len(data_files)}')\n",
    "    \n",
    "    # Lee el archivo .mseed\n",
    "    temp_df = read(file)\n",
    "    \n",
    "    # Extrae el ID del evento del nombre del archivo\n",
    "    evid_id = file.split('\\\\')[-1].split('_')[-1].split('evid')[1].split('.')[0]\n",
    "    \n",
    "    # Extrae el nombre del archivo sin la extensión .mseed\n",
    "    file_name = file.split('\\\\')[-1].rstrip('.mseed')\n",
    "    \n",
    "    # Verifica si el nombre del archivo existe en el DataFrame abstract_dfs\n",
    "    if len(abstract_dfs[abstract_dfs['filename'] == file_name]['time_rel(sec)']) == 0:\n",
    "        continue  # Salta este archivo si no existe en abstract_dfs\n",
    "    \n",
    "    # Obtiene el tiempo de inicio del DataFrame abstract_dfs\n",
    "    start = abstract_dfs[abstract_dfs['filename'] == file_name]['time_rel(sec)'].iloc[0]\n",
    "    \n",
    "    # Extrae la traza y los datos\n",
    "    tr = temp_df.traces[0].copy()\n",
    "    tr_data = tr.data  # Velocidades\n",
    "    tr_times = tr.times()  # Tiempos relativos\n",
    "\n",
    "    # Crea un diccionario temporal con la información requerida\n",
    "    temp_dict = {\n",
    "        'file_name': file_name, \n",
    "        'start': start,  \n",
    "        'id': evid_id, \n",
    "        'cant_measurements': temp_df[0].stats.npts,\n",
    "        'st': temp_df\n",
    "    }\n",
    "\n",
    "    # Agrega el diccionario temporal al DataFrame principal\n",
    "    df = pd.concat([df, pd.DataFrame([temp_dict])], ignore_index=True)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kono\\Desktop\\Code\\.venv\\Lib\\site-packages\\deap\\creator.py:185: RuntimeWarning: A class named 'FitnessMin' has already been created and it will be overwritten. Consider deleting previous creation of that class or rename it.\n",
      "  warnings.warn(\"A class named '{0}' has already been created and it \"\n",
      "c:\\Users\\Kono\\Desktop\\Code\\.venv\\Lib\\site-packages\\deap\\creator.py:185: RuntimeWarning: A class named 'Individual' has already been created and it will be overwritten. Consider deleting previous creation of that class or rename it.\n",
      "  warnings.warn(\"A class named '{0}' has already been created and it \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gen\tnevals\n",
      "0  \t50    \n"
     ]
    },
    {
     "ename": "InvalidParameterError",
     "evalue": "The 'max_depth' parameter of RandomForestRegressor must be an int in the range [1, inf) or None. Got 0 instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidParameterError\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[40], line 104\u001b[0m\n\u001b[0;32m    101\u001b[0m cxpb \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.5\u001b[39m\n\u001b[0;32m    102\u001b[0m mutpb \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.2\u001b[39m\n\u001b[1;32m--> 104\u001b[0m result, log \u001b[38;5;241m=\u001b[39m \u001b[43malgorithms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meaSimple\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpopulation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoolbox\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcxpb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmutpb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mngen\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    106\u001b[0m \u001b[38;5;66;03m# Get the best individual\u001b[39;00m\n\u001b[0;32m    107\u001b[0m best_individual \u001b[38;5;241m=\u001b[39m tools\u001b[38;5;241m.\u001b[39mselBest(population, k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\Kono\\Desktop\\Code\\.venv\\Lib\\site-packages\\deap\\algorithms.py:173\u001b[0m, in \u001b[0;36meaSimple\u001b[1;34m(population, toolbox, cxpb, mutpb, ngen, stats, halloffame, verbose)\u001b[0m\n\u001b[0;32m    171\u001b[0m invalid_ind \u001b[38;5;241m=\u001b[39m [ind \u001b[38;5;28;01mfor\u001b[39;00m ind \u001b[38;5;129;01min\u001b[39;00m offspring \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ind\u001b[38;5;241m.\u001b[39mfitness\u001b[38;5;241m.\u001b[39mvalid]\n\u001b[0;32m    172\u001b[0m fitnesses \u001b[38;5;241m=\u001b[39m toolbox\u001b[38;5;241m.\u001b[39mmap(toolbox\u001b[38;5;241m.\u001b[39mevaluate, invalid_ind)\n\u001b[1;32m--> 173\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mind\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfit\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43minvalid_ind\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfitnesses\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mind\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfitness\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mfit\u001b[49m\n\u001b[0;32m    176\u001b[0m \u001b[38;5;66;03m# Update the hall of fame with the generated individuals\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[40], line 66\u001b[0m, in \u001b[0;36mevaluate_model\u001b[1;34m(params)\u001b[0m\n\u001b[0;32m     58\u001b[0m n_estimators, max_depth, min_samples_split, min_samples_leaf \u001b[38;5;241m=\u001b[39m params\n\u001b[0;32m     59\u001b[0m model \u001b[38;5;241m=\u001b[39m RandomForestRegressor(\n\u001b[0;32m     60\u001b[0m     n_estimators\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mint\u001b[39m(n_estimators),\n\u001b[0;32m     61\u001b[0m     max_depth\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mint\u001b[39m(max_depth),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     64\u001b[0m     random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m\n\u001b[0;32m     65\u001b[0m )\n\u001b[1;32m---> 66\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_preprocessed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     67\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_test_preprocessed)\n\u001b[0;32m     68\u001b[0m mse \u001b[38;5;241m=\u001b[39m mean_squared_error(y_test, y_pred)\n",
      "File \u001b[1;32mc:\\Users\\Kono\\Desktop\\Code\\.venv\\Lib\\site-packages\\sklearn\\base.py:1466\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1461\u001b[0m partial_fit_and_fitted \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   1462\u001b[0m     fit_method\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpartial_fit\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m _is_fitted(estimator)\n\u001b[0;32m   1463\u001b[0m )\n\u001b[0;32m   1465\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m global_skip_validation \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m partial_fit_and_fitted:\n\u001b[1;32m-> 1466\u001b[0m     \u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_params\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1471\u001b[0m     )\n\u001b[0;32m   1472\u001b[0m ):\n\u001b[0;32m   1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Kono\\Desktop\\Code\\.venv\\Lib\\site-packages\\sklearn\\base.py:666\u001b[0m, in \u001b[0;36mBaseEstimator._validate_params\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    658\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_validate_params\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    659\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Validate types and values of constructor parameters\u001b[39;00m\n\u001b[0;32m    660\u001b[0m \n\u001b[0;32m    661\u001b[0m \u001b[38;5;124;03m    The expected type and values must be defined in the `_parameter_constraints`\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    664\u001b[0m \u001b[38;5;124;03m    accepted constraints.\u001b[39;00m\n\u001b[0;32m    665\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 666\u001b[0m     \u001b[43mvalidate_parameter_constraints\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    667\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parameter_constraints\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    668\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_params\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdeep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    669\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcaller_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__class__\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    670\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Kono\\Desktop\\Code\\.venv\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:95\u001b[0m, in \u001b[0;36mvalidate_parameter_constraints\u001b[1;34m(parameter_constraints, params, caller_name)\u001b[0m\n\u001b[0;32m     89\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     90\u001b[0m     constraints_str \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     91\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([\u001b[38;5;28mstr\u001b[39m(c)\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39mconstraints[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m or\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     92\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconstraints[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     93\u001b[0m     )\n\u001b[1;32m---> 95\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m InvalidParameterError(\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam_name\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m parameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcaller_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     97\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconstraints_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam_val\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     98\u001b[0m )\n",
      "\u001b[1;31mInvalidParameterError\u001b[0m: The 'max_depth' parameter of RandomForestRegressor must be an int in the range [1, inf) or None. Got 0 instead."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from deap import base, creator, tools, algorithms\n",
    "import numpy as np\n",
    "\n",
    "# Step 1: Load the data\n",
    "data = df  # Use the DataFrame 'df' directly\n",
    "\n",
    "# Step 2: Feature engineering on 'st'\n",
    "# Example: Extracting the number of points and sampling rate as features\n",
    "data['npts'] = data['st'].apply(lambda x: x[0].stats.npts if isinstance(x, list) and len(x) > 0 else 0)\n",
    "data['sampling_rate'] = data['st'].apply(lambda x: x[0].stats.sampling_rate if isinstance(x, list) and len(x) > 0 else 0)\n",
    "\n",
    "# Drop the original 'st' column after feature extraction\n",
    "data = data.drop(columns=['st'])\n",
    "\n",
    "# Step 3: Identify categorical columns\n",
    "categorical_cols = data.select_dtypes(include=['object']).columns\n",
    "\n",
    "# Step 4: Preprocess the data\n",
    "# Assuming 'start' is the column to predict and the rest are features\n",
    "X = data.drop(columns=['start'])\n",
    "y = data['start']\n",
    "\n",
    "# Handle missing values if any\n",
    "#X.fillna(X.mean(), inplace=True)\n",
    "\n",
    "# Step 5: Apply one-hot encoding to categorical columns\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), X.select_dtypes(include=['int64', 'float64']).columns),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_cols)\n",
    "    ])\n",
    "\n",
    "# Step 6: Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 7: Create a preprocessing pipeline\n",
    "preprocessing_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor)\n",
    "])\n",
    "\n",
    "# Preprocess the data\n",
    "X_train_preprocessed = preprocessing_pipeline.fit_transform(X_train)\n",
    "X_test_preprocessed = preprocessing_pipeline.transform(X_test)\n",
    "\n",
    "# Convert sparse matrix to dense matrix\n",
    "X_train_preprocessed = X_train_preprocessed.toarray()\n",
    "X_test_preprocessed = X_test_preprocessed.toarray()\n",
    "\n",
    "# Step 8: Define the genetic algorithm for optimizing Random Forest\n",
    "def evaluate_model(params):\n",
    "    n_estimators, max_depth, min_samples_split, min_samples_leaf = params\n",
    "    model = RandomForestRegressor(\n",
    "        n_estimators=int(n_estimators),\n",
    "        max_depth=int(max_depth),\n",
    "        min_samples_split=int(min_samples_split),\n",
    "        min_samples_leaf=int(min_samples_leaf),\n",
    "        random_state=42\n",
    "    )\n",
    "    model.fit(X_train_preprocessed, y_train)\n",
    "    y_pred = model.predict(X_test_preprocessed)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    return (mse,)\n",
    "\n",
    "# Define the parameter bounds\n",
    "param_bounds = {\n",
    "    'n_estimators': (10, 200),\n",
    "    'max_depth': (1, 20),\n",
    "    'min_samples_split': (2, 20),\n",
    "    'min_samples_leaf': (1, 20)\n",
    "}\n",
    "\n",
    "# Create the DEAP creator\n",
    "creator.create('FitnessMin', base.Fitness, weights=(-1.0,))\n",
    "creator.create('Individual', list, fitness=creator.FitnessMin)\n",
    "\n",
    "# Create the DEAP toolbox\n",
    "toolbox = base.Toolbox()\n",
    "toolbox.register('attr_n_estimators', np.random.randint, param_bounds['n_estimators'][0], param_bounds['n_estimators'][1])\n",
    "toolbox.register('attr_max_depth', np.random.randint, param_bounds['max_depth'][0], param_bounds['max_depth'][1])\n",
    "toolbox.register('attr_min_samples_split', np.random.randint, param_bounds['min_samples_split'][0], param_bounds['min_samples_split'][1])\n",
    "toolbox.register('attr_min_samples_leaf', np.random.randint, param_bounds['min_samples_leaf'][0], param_bounds['min_samples_leaf'][1])\n",
    "toolbox.register('individual', tools.initCycle, creator.Individual, \n",
    "                 (toolbox.attr_n_estimators, toolbox.attr_max_depth, toolbox.attr_min_samples_split, toolbox.attr_min_samples_leaf), n=1)\n",
    "toolbox.register('population', tools.initRepeat, list, toolbox.individual)\n",
    "toolbox.register('mate', tools.cxBlend, alpha=0.5)\n",
    "toolbox.register('mutate', tools.mutPolynomialBounded, low=[param_bounds['n_estimators'][0], param_bounds['max_depth'][0], param_bounds['min_samples_split'][0], param_bounds['min_samples_leaf'][0]], \n",
    "                 up=[param_bounds['n_estimators'][1], param_bounds['max_depth'][1], param_bounds['min_samples_split'][1], param_bounds['min_samples_leaf'][1]], eta=0.1, indpb=0.2)\n",
    "toolbox.register('select', tools.selTournament, tournsize=3)\n",
    "toolbox.register('evaluate', evaluate_model)\n",
    "\n",
    "# Run the genetic algorithm\n",
    "population = toolbox.population(n=50)\n",
    "ngen = 10\n",
    "cxpb = 0.5\n",
    "mutpb = 0.2\n",
    "\n",
    "result, log = algorithms.eaSimple(population, toolbox, cxpb, mutpb, ngen, verbose=True)\n",
    "\n",
    "# Get the best individual\n",
    "best_individual = tools.selBest(population, k=1)[0]\n",
    "best_params = [int(param) for param in best_individual]\n",
    "print(f'Best Parameters: {best_params}')\n",
    "\n",
    "# Train the final model with the best parameters\n",
    "best_model = RandomForestRegressor(\n",
    "    n_estimators=best_params[0],\n",
    "    max_depth=best_params[1],\n",
    "    min_samples_split=best_params[2],\n",
    "    min_samples_leaf=best_params[3],\n",
    "    random_state=42\n",
    ")\n",
    "best_model.fit(X_train_preprocessed, y_train)\n",
    "\n",
    "# Evaluate the final model\n",
    "y_pred = best_model.predict(X_test_preprocessed)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f'Mean Squared Error of Predictions: {mse}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
